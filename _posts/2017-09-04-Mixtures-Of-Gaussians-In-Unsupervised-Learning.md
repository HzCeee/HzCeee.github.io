---
layout: post
title: "Mixtures Of Gaussians In Unsupervised Learning"
author: "HzCeee"
categories: Machine Learning
tags: [Unsupervised Learning]
image:
  feature: 
  teaser: 
  credit:
  creditlink:
---

The mixture of Gaussians model posist that each $x^{(i)}$ was generated by randomly choosing $z^{(i)}$ from $\{ 1,2, \cdots, k \}$ and then $x^{(i)}$ was drawn from one of $k$ Gaussians depending on $z^{(i)}$.

Here, $z^{(i)} \sim Multinomial(\phi)$ where $\phi_j = p(z^{(i)} = j) \geq 0, \sum_{i = 1}^k \phi_j = 1$ and $x^{(i)} \| z^{(i)}  \sim \mathcal{N} (\mu_j, \Sigma_j)$.

If the $z^{(i)}$'s were known, the maximum likelihood estimation becomes identical to the Gaussian discriminant analysis model and $z^{(i)}$'s play the role of the class labels.

The log likelihood function is:

$$
l(\phi, \mu, \Sigma) = \sum_{i = 1}^m \log p(x^{(i)} | z^{(i)}; \mu, \Sigma) + \log p(z^{(i)} ; \phi)
$$

Maximizizing this with respect to $\phi, \mu, \Sigma$:

$$
\phi_j := \frac{1}{m} \sum_{i = 1}^m 1\{z^{(i)} = j\} \\\\
\mu_j := \frac{\sum_{i = 1}^m 1\{z^{(i)} = j\} x^{(i)}}{\sum_{i = 1}^m 1\{z^{(i)} = j\}} \\\\
\Sigma_j := \frac{\sum_{i = 1}^m 1\{z^{(i)} = j\} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i = 1}^m 1\{z^{(i)} = j\}} \\\\
$$

However, in unsupervised learning problem, the $z^{(i)}$'s are not known. Here is the EM algorithm:

$$
\begin{aligned}
&Initialize \ \phi,\mu,\Sigma \ randomly \\\\
&Repeat \ until \ convergence \ \{ \\\\
&\qquad (E-step) \ for \ i \ from \ 1 \ to \ m  \ \{ \\\\
&\qquad \qquad for \ j \ from \ 1 \ to \ k  \ \{ \\\\
&\qquad \qquad \qquad w^{(i)}_j := p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma) = \frac{p(x^{(i)} | z^{(i)} = j ; \mu, \Sigma)p(z^{(i)} = j ; \phi)}{\sum_{j = 1}^k p(x^{(i)} | z^{(i)} = j ; \mu, \Sigma)p(z^{(i)} = j ; \phi)} \\\\
&\qquad \qquad \} \\\\
&\qquad \} \\\\
&\qquad (M-step) for \ j \ from \ 1 \ to \ k  \ \{ \\\\
&\qquad \qquad \phi_j := \frac{1}{m} \sum_{i = 1}^m w_j^{(i)} \\\\
&\qquad \qquad \mu_j := \frac{\sum_{i = 1}^m w^{(i)}_j x^{(i)}}{\sum_{i = 1}^m w^{(i)}_j} \\\\
&\qquad \qquad \Sigma_j := \frac{\sum_{i = 1}^m w^{(i)}_j (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i = 1}^m w^{(i)}_j} \\\\
&\qquad \} \\\\
&\} \\\\
\end{aligned}
$$